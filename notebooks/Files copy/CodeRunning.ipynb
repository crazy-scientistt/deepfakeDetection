{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing import image \n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✱] Cleared existing files in 'frames'\n",
      "[✓] Saved frames/face_0_0.jpg\n",
      "[✓] Saved frames/face_20_0.jpg\n",
      "[✓] Saved frames/face_40_0.jpg\n",
      "[✓] Saved frames/face_60_0.jpg\n",
      "[✓] Saved frames/face_80_0.jpg\n",
      "[✓] Saved frames/face_100_0.jpg\n",
      "[✓] Saved frames/face_120_0.jpg\n",
      "[✓] Saved frames/face_140_0.jpg\n",
      "[✓] Saved frames/face_160_0.jpg\n",
      "[✓] Saved frames/face_180_0.jpg\n",
      "[✓] Saved frames/face_200_0.jpg\n",
      "[✓] Saved frames/face_220_0.jpg\n",
      "[✓] Saved frames/face_240_0.jpg\n",
      "[✓] Saved frames/face_260_0.jpg\n",
      "[✓] Saved frames/face_260_1.jpg\n",
      "[✓] Saved frames/face_280_0.jpg\n",
      "[✓] Saved frames/face_280_1.jpg\n",
      "[✓] Saved frames/face_280_2.jpg\n",
      "[✓] Saved frames/face_300_0.jpg\n",
      "[✓] Saved frames/face_300_1.jpg\n",
      "[✓] Saved frames/face_300_2.jpg\n",
      "[✓] Saved frames/face_320_0.jpg\n",
      "[✓] Saved frames/face_320_1.jpg\n",
      "[✓] Saved frames/face_320_2.jpg\n",
      "[✓] Saved frames/face_320_3.jpg\n",
      "[✓] Saved frames/face_340_0.jpg\n",
      "[✓] Saved frames/face_340_1.jpg\n",
      "[✓] Saved frames/face_340_2.jpg\n",
      "[✓] Saved frames/face_360_0.jpg\n",
      "[✓] Saved frames/face_360_1.jpg\n",
      "[✓] Saved frames/face_360_2.jpg\n",
      "[✓] Saved frames/face_360_3.jpg\n",
      "[✓] Saved frames/face_360_4.jpg\n",
      "[✓] Saved frames/face_360_5.jpg\n",
      "[✓] Saved frames/face_380_0.jpg\n",
      "[✓] Saved frames/face_380_1.jpg\n",
      "[✓] Saved frames/face_380_2.jpg\n",
      "[✓] Saved frames/face_400_0.jpg\n",
      "[✓] Saved frames/face_400_1.jpg\n",
      "[✓] Saved frames/face_400_2.jpg\n",
      "[✓] Saved frames/face_400_3.jpg\n",
      "[✓] Saved frames/face_420_0.jpg\n",
      "[✓] Saved frames/face_420_1.jpg\n",
      "[✓] Saved frames/face_420_2.jpg\n",
      "[✓] Saved frames/face_440_0.jpg\n",
      "[✓] Saved frames/face_440_1.jpg\n",
      "[✓] Saved frames/face_440_2.jpg\n",
      "[✓] Saved frames/face_460_0.jpg\n",
      "[✓] Saved frames/face_460_1.jpg\n",
      "[✓] Saved frames/face_460_2.jpg\n",
      "[✓] Saved frames/face_460_3.jpg\n",
      "[✓] Saved frames/face_480_0.jpg\n",
      "[✓] Saved frames/face_480_1.jpg\n",
      "[✓] Saved frames/face_480_2.jpg\n",
      "[✓] Saved frames/face_500_0.jpg\n",
      "[✓] Saved frames/face_500_1.jpg\n",
      "[✓] Saved frames/face_500_2.jpg\n",
      "[✓] Saved frames/face_500_3.jpg\n",
      "[✓] Saved frames/face_500_4.jpg\n",
      "[✓] Saved frames/face_500_5.jpg\n",
      "[✓] Saved frames/face_520_0.jpg\n",
      "[✓] Saved frames/face_540_0.jpg\n",
      "[✓] Saved frames/face_560_0.jpg\n",
      "[✓] Saved frames/face_580_0.jpg\n",
      "[✓] Saved frames/face_600_0.jpg\n",
      "[✓] Saved frames/face_620_0.jpg\n",
      "[✓] Saved frames/face_640_0.jpg\n",
      "[✓] Saved frames/face_660_0.jpg\n",
      "[✓] Saved frames/face_680_0.jpg\n",
      "[✓] Saved frames/face_700_0.jpg\n",
      "[✓] Saved frames/face_720_0.jpg\n",
      "[✓] Saved frames/face_740_0.jpg\n",
      "[✓] Saved frames/face_740_1.jpg\n",
      "[✓] Saved frames/face_760_0.jpg\n",
      "[✓] Saved frames/face_780_0.jpg\n",
      "[✓] Saved frames/face_800_0.jpg\n",
      "[✓] Saved frames/face_820_0.jpg\n",
      "[✓] Saved frames/face_840_0.jpg\n",
      "[✓] Saved frames/face_860_0.jpg\n",
      "[✓] Saved frames/face_880_0.jpg\n",
      "[✓] Saved frames/face_900_0.jpg\n",
      "[✓] Saved frames/face_920_0.jpg\n",
      "[✓] Saved frames/face_940_0.jpg\n",
      "[✓] Saved frames/face_960_0.jpg\n",
      "[✓] Saved frames/face_980_0.jpg\n",
      "[✓] Saved frames/face_1000_0.jpg\n",
      "[✓] Saved frames/face_1020_0.jpg\n",
      "[✓] Saved frames/face_1060_0.jpg\n",
      "[✓] Saved frames/face_1080_0.jpg\n",
      "[✓] Saved frames/face_1100_0.jpg\n",
      "[✓] Saved frames/face_1120_0.jpg\n",
      "[✓] Saved frames/face_1140_0.jpg\n",
      "[✓] Saved frames/face_1220_0.jpg\n",
      "[✓] Saved frames/face_1260_0.jpg\n",
      "[✓] Saved frames/face_1340_0.jpg\n",
      "[✓] Saved frames/face_1340_1.jpg\n",
      "[✓] Saved frames/face_1360_0.jpg\n",
      "[✓] Saved frames/face_1360_1.jpg\n",
      "[✓] Saved frames/face_1380_0.jpg\n",
      "[✓] Saved frames/face_1380_1.jpg\n",
      "[✓] Saved frames/face_1400_0.jpg\n",
      "[✓] Saved frames/face_1400_1.jpg\n",
      "[✓] Saved frames/face_1420_0.jpg\n",
      "[✓] Saved frames/face_1420_1.jpg\n",
      "[✓] Saved frames/face_1440_0.jpg\n",
      "[✓] Saved frames/face_1440_1.jpg\n",
      "[✓] Saved frames/face_1460_0.jpg\n",
      "[✓] Saved frames/face_1460_1.jpg\n",
      "[✓] Saved frames/face_1480_0.jpg\n",
      "[✓] Saved frames/face_1480_1.jpg\n",
      "[✓] Saved frames/face_1500_0.jpg\n",
      "[✓] Saved frames/face_1500_1.jpg\n",
      "[✓] Saved frames/face_1520_0.jpg\n",
      "[✓] Saved frames/face_1520_1.jpg\n",
      "[✓] Saved frames/face_1540_0.jpg\n",
      "[✓] Saved frames/face_1540_1.jpg\n",
      "[✓] Saved frames/face_1560_0.jpg\n",
      "\n",
      "Done: 117 face crops saved.\n",
      "No faces detected, removing: face_1260_0.jpg\n",
      "No faces detected, removing: face_1340_0.jpg\n",
      "Duplicate found and removing: face_1360_0.jpg\n",
      "No faces detected, removing: face_1360_1.jpg\n",
      "Duplicate found and removing: face_1380_0.jpg\n",
      "No faces detected, removing: face_1380_1.jpg\n",
      "Duplicate found and removing: face_1400_0.jpg\n",
      "No faces detected, removing: face_1400_1.jpg\n",
      "Duplicate found and removing: face_1420_0.jpg\n",
      "No faces detected, removing: face_1420_1.jpg\n",
      "No faces detected, removing: face_1440_1.jpg\n",
      "Duplicate found and removing: face_1460_0.jpg\n",
      "No faces detected, removing: face_1460_1.jpg\n",
      "Duplicate found and removing: face_1480_0.jpg\n",
      "No faces detected, removing: face_1480_1.jpg\n",
      "Duplicate found and removing: face_1500_0.jpg\n",
      "No faces detected, removing: face_1500_1.jpg\n",
      "Duplicate found and removing: face_1520_0.jpg\n",
      "No faces detected, removing: face_1520_1.jpg\n",
      "Duplicate found and removing: face_1540_0.jpg\n",
      "No faces detected, removing: face_1540_1.jpg\n",
      "No faces detected, removing: face_1560_0.jpg\n",
      "No faces detected, removing: face_260_0.jpg\n",
      "No faces detected, removing: face_280_2.jpg\n",
      "No faces detected, removing: face_300_0.jpg\n",
      "Duplicate found and removing: face_340_0.jpg\n",
      "No faces detected, removing: face_340_1.jpg\n",
      "Duplicate found and removing: face_360_0.jpg\n",
      "No faces detected, removing: face_360_2.jpg\n",
      "No faces detected, removing: face_360_5.jpg\n",
      "No faces detected, removing: face_400_1.jpg\n",
      "No faces detected, removing: face_420_2.jpg\n",
      "No faces detected, removing: face_440_0.jpg\n",
      "No faces detected, removing: face_440_1.jpg\n",
      "No faces detected, removing: face_440_2.jpg\n",
      "No faces detected, removing: face_460_1.jpg\n",
      "No faces detected, removing: face_480_1.jpg\n",
      "No faces detected, removing: face_500_1.jpg\n",
      "No faces detected, removing: face_500_2.jpg\n",
      "No faces detected, removing: face_500_3.jpg\n",
      "No faces detected, removing: face_740_0.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "import cv2\n",
    "\n",
    "# Path to the folder containing frames\n",
    "frames_directory = r\"C:\\Users\\creat\\Desktop\\semesters\\7th semester\\deepfake_fyp1\\notebooks\\frames\"\n",
    "\n",
    "# Load Haar cascade face detector\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def FrameCapture(video_path, output_dir=\"frames\", frame_skip=20, min_face_size=60):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Cannot open video.\")\n",
    "        return\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    frame_count = 0\n",
    "    saved_faces = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_count % frame_skip == 0:\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(\n",
    "                gray,\n",
    "                scaleFactor=1.1,\n",
    "                minNeighbors=6,\n",
    "                minSize=(min_face_size, min_face_size)\n",
    "            )\n",
    "\n",
    "            for i, (x, y, w, h) in enumerate(faces):\n",
    "                aspect_ratio = w / float(h)\n",
    "                if aspect_ratio < 0.75 or aspect_ratio > 1.33:\n",
    "                    continue\n",
    "\n",
    "                face = frame[y:y+h, x:x+w]\n",
    "                brightness = face.mean()\n",
    "                if brightness < 40:\n",
    "                    continue\n",
    "\n",
    "                filename = f\"{output_dir}/face_{frame_count}_{i}.jpg\"\n",
    "                cv2.imwrite(filename, face)\n",
    "\n",
    "                print(f\"[✓] Saved {filename}\")\n",
    "                saved_faces += 1\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"\\nDone: {saved_faces} face crops saved.\")\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff', '.gif'))\n",
    "\n",
    "def process_file(path, output_dir=\"frames\", min_face_size=60):\n",
    "    # Clear existing frames\n",
    "    if os.path.exists(output_dir):\n",
    "        for f in os.listdir(output_dir):\n",
    "            file_path = os.path.join(output_dir, f)\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "        print(f\"[✱] Cleared existing files in '{output_dir}'\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    if is_image_file(path):\n",
    "        img = cv2.imread(path)\n",
    "        if img is None:\n",
    "            print(\"Error: Cannot read image.\")\n",
    "            return\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(\n",
    "            gray,\n",
    "            scaleFactor=1.1,\n",
    "            minNeighbors=6,\n",
    "            minSize=(min_face_size, min_face_size)\n",
    "        )\n",
    "\n",
    "        total_saved = 0\n",
    "        for i, (x, y, w, h) in enumerate(faces):\n",
    "            aspect_ratio = w / float(h)\n",
    "            if aspect_ratio < 0.75 or aspect_ratio > 1.33:\n",
    "                continue\n",
    "\n",
    "            face = img[y:y+h, x:x+w]\n",
    "            brightness = face.mean()\n",
    "            if brightness < 40:\n",
    "                continue\n",
    "\n",
    "            # Save 30 copies of the face\n",
    "            for j in range(30):\n",
    "                filename = f\"{output_dir}/image_face_{i}_copy_{j}.jpg\"\n",
    "                cv2.imwrite(filename, face)\n",
    "\n",
    "                print(f\"[✓] Saved {filename}\")\n",
    "                total_saved += 1\n",
    "\n",
    "        print(f\"[→] Detected {len(faces)} valid face(s)\")\n",
    "        print(f\"[→] Saved {total_saved} face crops (30 per face)\")\n",
    "    else:\n",
    "        FrameCapture(path)\n",
    "\n",
    "\n",
    "def remove_non_face_and_duplicate_frames(directory):\n",
    "    # Set to store unique hashes of images\n",
    "    seen_hashes = set()\n",
    "\n",
    "    # List all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "\n",
    "    # Iterate over all files\n",
    "    for file in files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "\n",
    "        # Only consider image files (you can extend this list if needed)\n",
    "        if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "            try:\n",
    "                # Open image and compute hash\n",
    "                image = Image.open(file_path)\n",
    "                image_hash = imagehash.average_hash(image)\n",
    "\n",
    "                # Detect faces in the image\n",
    "                img_cv = cv2.imread(file_path)\n",
    "                gray = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY)\n",
    "                faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "                # If no faces are detected, remove the image\n",
    "                if len(faces) == 0:\n",
    "                    print(f\"No faces detected, removing: {file}\")\n",
    "                    os.remove(file_path)\n",
    "                else:\n",
    "                    # Check if the image hash is already in the set (duplicate)\n",
    "                    if image_hash in seen_hashes:\n",
    "                        print(f\"Duplicate found and removing: {file}\")\n",
    "                        os.remove(file_path)\n",
    "                    else:\n",
    "                        # Otherwise, add the hash to the set (unique image with face)\n",
    "                        seen_hashes.add(image_hash)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "process_file(r\"C:\\Users\\creat\\Downloads\\real video 3.mp4\")\n",
    "remove_non_face_and_duplicate_frames(frames_directory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load your saved Xception-based model\n",
    "loaded_model = load_model(r\"C:\\Users\\creat\\Desktop\\semesters\\7th semester\\deepfake_fyp1\\models/XceptionSoftmax.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-frame predictions (Fake class confidence):\n",
      "  Frame 1: 0.5023\n",
      "  Frame 2: 0.4077\n",
      "  Frame 3: 0.5426\n",
      "  Frame 4: 0.5376\n",
      "  Frame 5: 0.3807\n",
      "  Frame 6: 0.5247\n",
      "  Frame 7: 0.5122\n",
      "  Frame 8: 0.6733\n",
      "  Frame 9: 0.7042\n",
      "  Frame 10: 0.6544\n",
      "  Frame 11: 0.3799\n",
      "  Frame 12: 0.5294\n",
      "  Frame 13: 0.5875\n",
      "  Frame 14: 0.4590\n",
      "  Frame 15: 0.5353\n",
      "  Frame 16: 0.5670\n",
      "  Frame 17: 0.6092\n",
      "  Frame 18: 0.5709\n",
      "  Frame 19: 0.4799\n",
      "  Frame 20: 0.5064\n",
      "  Frame 21: 0.5380\n",
      "  Frame 22: 0.9129\n",
      "  Frame 23: 0.3094\n",
      "  Frame 24: 0.9785\n",
      "  Frame 25: 0.7564\n",
      "  Frame 26: 0.9431\n",
      "  Frame 27: 0.4685\n",
      "  Frame 28: 0.5267\n",
      "  Frame 29: 0.5740\n",
      "  Frame 30: 0.6017\n",
      "  Frame 31: 0.6469\n",
      "  Frame 32: 0.2842\n",
      "  Frame 33: 0.1024\n",
      "  Frame 34: 0.2514\n",
      "  Frame 35: 0.0871\n",
      "  Frame 36: 0.6031\n",
      "  Frame 37: 0.5883\n",
      "  Frame 38: 0.2611\n",
      "  Frame 39: 0.7861\n",
      "  Frame 40: 0.7791\n",
      "  Frame 41: 0.4763\n",
      "  Frame 42: 0.6461\n",
      "  Frame 43: 0.7012\n",
      "  Frame 44: 0.8415\n",
      "  Frame 45: 0.7238\n",
      "  Frame 46: 0.5825\n",
      "  Frame 47: 0.8558\n",
      "  Frame 48: 0.7035\n",
      "  Frame 49: 0.5913\n",
      "  Frame 50: 0.4072\n",
      "  Frame 51: 0.6812\n",
      "  Frame 52: 0.5981\n",
      "  Frame 53: 0.7509\n",
      "  Frame 54: 0.6677\n",
      "  Frame 55: 0.4152\n",
      "  Frame 56: 0.8636\n",
      "  Frame 57: 0.6562\n",
      "  Frame 58: 0.9493\n",
      "  Frame 59: 0.6552\n",
      "  Frame 60: 0.7405\n",
      "  Frame 61: 0.5162\n",
      "  Frame 62: 0.6217\n",
      "  Frame 63: 0.6899\n",
      "  Frame 64: 0.8655\n",
      "  Frame 65: 0.7001\n",
      "  Frame 66: 0.5595\n",
      "  Frame 67: 0.8463\n",
      "  Frame 68: 0.6389\n",
      "  Frame 69: 0.6997\n",
      "  Frame 70: 0.5981\n",
      "  Frame 71: 0.6175\n",
      "  Frame 72: 0.6505\n",
      "  Frame 73: 0.5568\n",
      "\n",
      "Final Prediction: Fake (Average Confidence: 0.5990)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def evaluate_video_sequence(directory, loaded_model, input_size=(224, 224)):\n",
    "    frame_files = sorted([f for f in os.listdir(directory) if f.endswith(('.jpg', '.png'))])\n",
    "    \n",
    "    if not frame_files:\n",
    "        print(\"No image frames found in the directory.\")\n",
    "        return\n",
    "\n",
    "    frames = []\n",
    "    for filename in frame_files:\n",
    "        img_path = os.path.join(directory, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print(f\"Warning: Couldn't read {filename}, skipping.\")\n",
    "            continue\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, input_size)\n",
    "        img = preprocess_input(img.astype(np.float32))\n",
    "        frames.append(img)\n",
    "\n",
    "    if not frames:\n",
    "        print(\"No valid frames to process.\")\n",
    "        return\n",
    "\n",
    "    input_batch = np.array(frames)\n",
    "\n",
    "    # Predict on each frame\n",
    "    predictions = loaded_model.predict(input_batch, verbose=0)  # shape: (num_frames, 2)\n",
    "    \n",
    "    # Print confidence for each frame\n",
    "    print(\"Per-frame predictions (Fake class confidence):\")\n",
    "    for i, pred in enumerate(predictions):\n",
    "        print(f\"  Frame {i + 1}: {pred[1]:.4f}\")\n",
    "\n",
    "    # Take average prediction for class 1 (e.g., \"Fake\")\n",
    "    confidence = np.mean(predictions[:, 1])\n",
    "    label = \"Fake\" if confidence >= 0.5 else \"Real\"\n",
    "    print(f\"\\nFinal Prediction: {label} (Average Confidence: {confidence:.4f})\")\n",
    "\n",
    "# Usage example (ensure your model is loaded):\n",
    "# loaded_model = load_model(\"deepfake_model.h5\")\n",
    "evaluate_video_sequence(\"frames\", loaded_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory cleared!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gc\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    # Clear TensorFlow session\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    # Collect garbage\n",
    "    gc.collect()\n",
    "    \n",
    "    # Reset the default graph\n",
    "    ops.reset_default_graph()\n",
    "\n",
    "    print(\"GPU memory cleared!\")\n",
    "\n",
    "# Call this function when you need to clear the memory\n",
    "clear_gpu_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
